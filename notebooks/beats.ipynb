{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import torch\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "\n",
    "from BEATs.Tokenizers import TokenizersConfig, Tokenizers\n",
    "from BEATs.BEATs import BEATs, BEATsConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BEATs with the provided code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('/data/BEATs/Tokenizer_iter3_plus_AS2M.pt')\n",
    "\n",
    "cfg = TokenizersConfig(checkpoint['cfg'])\n",
    "BEATs_tokenizer = Tokenizers(cfg)\n",
    "BEATs_tokenizer.load_state_dict(checkpoint['model'])\n",
    "BEATs_tokenizer.eval()\n",
    "\n",
    "# tokenize the audio and generate the labels\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "labels = BEATs_tokenizer.extract_labels(audio_input_16khz, padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('/data/BEATs/BEATs_iter3_plus_AS2M.pt')\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# extract the the audio representation\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "representation = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned checkpoints\n",
    "checkpoint = torch.load('/data/BEATs/BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt')\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# predict the classification probability of each class\n",
    "audio_input_16khz = torch.randn(3, 10000)\n",
    "padding_mask = torch.zeros(3, 10000).bool()\n",
    "\n",
    "probs = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]\n",
    "\n",
    "for i, (top5_label_prob, top5_label_idx) in enumerate(zip(*probs.topk(k=5))):\n",
    "    top5_label = [checkpoint['label_dict'][label_idx.item()] for label_idx in top5_label_idx]\n",
    "    print(f'Top 5 predicted labels of the {i}th audio are {top5_label} with probability of {top5_label_prob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEATs_model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BEATs with my own files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "data_folder = \"/data/different_bird_songs/\"\n",
    "data = glob.glob(data_folder + \"/**/*.mp3\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and resample at 16000Hz\n",
    "trs = []\n",
    "l = []\n",
    "\n",
    "for afile in data:\n",
    "    sig, sr = librosa.load(afile, sr = 16000, mono=True)\n",
    "    sig_t = torch.tensor(sig).unsqueeze(0)\n",
    "    trs.append(sig_t)\n",
    "    l.append(afile.split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained checkpoints for the tokenizer\n",
    "checkpoint = torch.load('/data/BEATs/Tokenizer_iter3_plus_AS2M.pt')\n",
    "cfg = TokenizersConfig(checkpoint['cfg'])\n",
    "BEATs_tokenizer = Tokenizers(cfg)\n",
    "BEATs_tokenizer.load_state_dict(checkpoint['model'])\n",
    "BEATs_tokenizer.eval()\n",
    "\n",
    "# tokenize the audio and generate the labels\n",
    "labels = BEATs_tokenizer.extract_labels(trs[0], padding_mask=padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('/data/BEATs/BEATs_iter3_plus_AS2M.pt')\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()\n",
    "\n",
    "# extract the the audio representation\n",
    "l_representations = []\n",
    "\n",
    "for t in trs:\n",
    "    padding_mask = torch.zeros(t.shape[0], t.shape[1]).bool()\n",
    "    representation = BEATs_model.extract_features(t, padding_mask=padding_mask)[0]\n",
    "    # DIMENSIONS are: Batch / Number of labels / Audio encoded in 768 dimension\n",
    "    l_representations.append(representation[:,-1,:]) # Take only the last dimension as this is the encoded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "representation = torch.cat(l_representations, dim=0)\n",
    "representation = representation.detach().numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "representation_2d = tsne.fit_transform(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x = representation_2d[:, 0], y = representation_2d[:, 1], hue = l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "representation_2d_pca = tsne.fit_transform(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
